{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52cb4eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1031175 entries, 0 to 1031174\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count    Dtype  \n",
      "---  ------               --------------    -----  \n",
      " 0   user_id              1031175 non-null  int64  \n",
      " 1   location             1031175 non-null  object \n",
      " 2   age                  1031175 non-null  float64\n",
      " 3   isbn                 1031175 non-null  object \n",
      " 4   rating               1031175 non-null  int64  \n",
      " 5   book_title           1031175 non-null  object \n",
      " 6   book_author          1031175 non-null  object \n",
      " 7   year_of_publication  1031175 non-null  float64\n",
      " 8   publisher            1031175 non-null  object \n",
      " 9   img_s                1031175 non-null  object \n",
      " 10  img_m                1031175 non-null  object \n",
      " 11  img_l                1031175 non-null  object \n",
      " 12  Summary              1031175 non-null  object \n",
      " 13  Language             1031175 non-null  object \n",
      " 14  Category             1031175 non-null  object \n",
      " 15  city                 1017072 non-null  object \n",
      " 16  state                1008377 non-null  object \n",
      " 17  country              995801 non-null   object \n",
      "dtypes: float64(2), int64(2), object(14)\n",
      "memory usage: 141.6+ MB\n",
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "{'book_author': b'Mark P. O. Morford',\n",
      " 'book_title': b'Classical Mythology',\n",
      " 'rating': 0.0,\n",
      " 'user_id': b'2'}\n",
      "Epoch 1/25\n",
      "293/293 [==============================] - 9s 7ms/step - root_mean_squared_error: 3.8170 - loss: 14.5657 - regularization_loss: 0.0000e+00 - total_loss: 14.5657 - val_root_mean_squared_error: 3.6939 - val_loss: 13.7062 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13.7062\n",
      "Epoch 2/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 3.3834 - loss: 11.4454 - regularization_loss: 0.0000e+00 - total_loss: 11.4454 - val_root_mean_squared_error: 3.8452 - val_loss: 15.0566 - val_regularization_loss: 0.0000e+00 - val_total_loss: 15.0566\n",
      "Epoch 3/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 2.9803 - loss: 8.8791 - regularization_loss: 0.0000e+00 - total_loss: 8.8791 - val_root_mean_squared_error: 3.9334 - val_loss: 15.2937 - val_regularization_loss: 0.0000e+00 - val_total_loss: 15.2937\n",
      "Epoch 4/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 2.6170 - loss: 6.8479 - regularization_loss: 0.0000e+00 - total_loss: 6.8479 - val_root_mean_squared_error: 4.0099 - val_loss: 15.7685 - val_regularization_loss: 0.0000e+00 - val_total_loss: 15.7685\n",
      "Epoch 5/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 2.3862 - loss: 5.6927 - regularization_loss: 0.0000e+00 - total_loss: 5.6927 - val_root_mean_squared_error: 4.1787 - val_loss: 16.5789 - val_regularization_loss: 0.0000e+00 - val_total_loss: 16.5789\n",
      "Epoch 6/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 2.2644 - loss: 5.1260 - regularization_loss: 0.0000e+00 - total_loss: 5.1260 - val_root_mean_squared_error: 4.2320 - val_loss: 17.2658 - val_regularization_loss: 0.0000e+00 - val_total_loss: 17.2658\n",
      "Epoch 7/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 2.1740 - loss: 4.7264 - regularization_loss: 0.0000e+00 - total_loss: 4.7264 - val_root_mean_squared_error: 4.2462 - val_loss: 17.4446 - val_regularization_loss: 0.0000e+00 - val_total_loss: 17.4446\n",
      "Epoch 8/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 2.1001 - loss: 4.4106 - regularization_loss: 0.0000e+00 - total_loss: 4.4106 - val_root_mean_squared_error: 4.3331 - val_loss: 17.8586 - val_regularization_loss: 0.0000e+00 - val_total_loss: 17.8586\n",
      "Epoch 9/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 2.0478 - loss: 4.1950 - regularization_loss: 0.0000e+00 - total_loss: 4.1950 - val_root_mean_squared_error: 4.3358 - val_loss: 18.0428 - val_regularization_loss: 0.0000e+00 - val_total_loss: 18.0428\n",
      "Epoch 10/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 2.0069 - loss: 4.0292 - regularization_loss: 0.0000e+00 - total_loss: 4.0292 - val_root_mean_squared_error: 4.4167 - val_loss: 18.2355 - val_regularization_loss: 0.0000e+00 - val_total_loss: 18.2355\n",
      "Epoch 11/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.9899 - loss: 3.9612 - regularization_loss: 0.0000e+00 - total_loss: 3.9612 - val_root_mean_squared_error: 4.4570 - val_loss: 18.8056 - val_regularization_loss: 0.0000e+00 - val_total_loss: 18.8056\n",
      "Epoch 12/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.9393 - loss: 3.7623 - regularization_loss: 0.0000e+00 - total_loss: 3.7623 - val_root_mean_squared_error: 4.5147 - val_loss: 19.5255 - val_regularization_loss: 0.0000e+00 - val_total_loss: 19.5255\n",
      "Epoch 13/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.8912 - loss: 3.5786 - regularization_loss: 0.0000e+00 - total_loss: 3.5786 - val_root_mean_squared_error: 4.5124 - val_loss: 19.6275 - val_regularization_loss: 0.0000e+00 - val_total_loss: 19.6275\n",
      "Epoch 14/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.8582 - loss: 3.4545 - regularization_loss: 0.0000e+00 - total_loss: 3.4545 - val_root_mean_squared_error: 4.5501 - val_loss: 20.2048 - val_regularization_loss: 0.0000e+00 - val_total_loss: 20.2048\n",
      "Epoch 15/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.8250 - loss: 3.3323 - regularization_loss: 0.0000e+00 - total_loss: 3.3323 - val_root_mean_squared_error: 4.5791 - val_loss: 20.1650 - val_regularization_loss: 0.0000e+00 - val_total_loss: 20.1650\n",
      "Epoch 16/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.7855 - loss: 3.1895 - regularization_loss: 0.0000e+00 - total_loss: 3.1895 - val_root_mean_squared_error: 4.6289 - val_loss: 21.0700 - val_regularization_loss: 0.0000e+00 - val_total_loss: 21.0700\n",
      "Epoch 17/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.7528 - loss: 3.0736 - regularization_loss: 0.0000e+00 - total_loss: 3.0736 - val_root_mean_squared_error: 4.6700 - val_loss: 21.2751 - val_regularization_loss: 0.0000e+00 - val_total_loss: 21.2751\n",
      "Epoch 18/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.7060 - loss: 2.9120 - regularization_loss: 0.0000e+00 - total_loss: 2.9120 - val_root_mean_squared_error: 4.6618 - val_loss: 21.5347 - val_regularization_loss: 0.0000e+00 - val_total_loss: 21.5347\n",
      "Epoch 19/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.6809 - loss: 2.8265 - regularization_loss: 0.0000e+00 - total_loss: 2.8265 - val_root_mean_squared_error: 4.7312 - val_loss: 22.2542 - val_regularization_loss: 0.0000e+00 - val_total_loss: 22.2542\n",
      "Epoch 20/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.6389 - loss: 2.6874 - regularization_loss: 0.0000e+00 - total_loss: 2.6874 - val_root_mean_squared_error: 4.7093 - val_loss: 22.7964 - val_regularization_loss: 0.0000e+00 - val_total_loss: 22.7964\n",
      "Epoch 21/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.6088 - loss: 2.5894 - regularization_loss: 0.0000e+00 - total_loss: 2.5894 - val_root_mean_squared_error: 4.7358 - val_loss: 22.8841 - val_regularization_loss: 0.0000e+00 - val_total_loss: 22.8841\n",
      "Epoch 22/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.5734 - loss: 2.4771 - regularization_loss: 0.0000e+00 - total_loss: 2.4771 - val_root_mean_squared_error: 4.7527 - val_loss: 23.3702 - val_regularization_loss: 0.0000e+00 - val_total_loss: 23.3702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.5351 - loss: 2.3577 - regularization_loss: 0.0000e+00 - total_loss: 2.3577 - val_root_mean_squared_error: 4.7728 - val_loss: 23.4948 - val_regularization_loss: 0.0000e+00 - val_total_loss: 23.4948\n",
      "Epoch 24/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.4968 - loss: 2.2419 - regularization_loss: 0.0000e+00 - total_loss: 2.2419 - val_root_mean_squared_error: 4.7902 - val_loss: 24.1502 - val_regularization_loss: 0.0000e+00 - val_total_loss: 24.1502\n",
      "Epoch 25/25\n",
      "293/293 [==============================] - 1s 3ms/step - root_mean_squared_error: 1.4601 - loss: 2.1332 - regularization_loss: 0.0000e+00 - total_loss: 2.1332 - val_root_mean_squared_error: 4.7805 - val_loss: 24.0328 - val_regularization_loss: 0.0000e+00 - val_total_loss: 24.0328\n",
      "98/98 [==============================] - 0s 1ms/step - root_mean_squared_error: 4.7805 - loss: 22.8694 - regularization_loss: 0.0000e+00 - total_loss: 22.8694\n",
      " Beyond IBM: Leadership Marketing and Finance for the 1990s: [[5.578377]]\n",
      " Earth Prayers From around the World: 365 Prayers, Poems, and Invocations for Honoring the Earth: [[5.5655074]]\n",
      " Garfield Bigger and Better (Garfield (Numbered Paperback)): [[5.560525]]\n",
      " Flight of Fancy: American Heiresses (Zebra Ballad Romance): [[5.5161324]]\n",
      " Good Wives: Image and Reality in the Lives of Women in Northern New England, 1650-1750: [[5.2425184]]\n",
      " Clifford Visita El Hospital (Clifford El Gran Perro Colorado): [[5.0493646]]\n",
      " Dark Justice: [[5.007474]]\n",
      " Always Have Popsicles: [[4.969739]]\n",
      " Deceived: [[4.840076]]\n",
      " Goosebumps Monster Edition 1: Welcome to Dead House, Stay Out of the Basement, and Say Cheese and Die!: [[4.5778375]]\n",
      " Ask Lily (Young Women of Faith: Lily Series, Book 5): [[4.471164]]\n",
      " Final Fantasy Anthology: Official Strategy Guide (Brady Games): [[4.4461675]]\n",
      " God's Little Promise Book: [[4.258909]]\n",
      " A Light in the Storm: The Civil War Diary of Amelia Martin, Fenwick Island, Delaware, 1861 (Dear America): [[3.8476915]]\n",
      " Apple Magic (The Collector's series): [[3.7979202]]\n",
      "user_id              254     2276    2766    2977    3363    4017    4385    \\\n",
      "book_title                                                                    \n",
      "1984                    9.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1st to Die: A Novel     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2nd Chance              0.0    10.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4 Blondes               0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "A Bend in the Road      0.0     0.0     7.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "user_id              6251    6323    6543    ...  271705  273979  274004  \\\n",
      "book_title                                   ...                           \n",
      "1984                    0.0     0.0     0.0  ...    10.0     0.0     0.0   \n",
      "1st to Die: A Novel     0.0     0.0     9.0  ...     0.0     0.0     0.0   \n",
      "2nd Chance              0.0     0.0     0.0  ...     0.0     0.0     0.0   \n",
      "4 Blondes               0.0     0.0     0.0  ...     0.0     0.0     0.0   \n",
      "A Bend in the Road      0.0     0.0     0.0  ...     0.0     0.0     0.0   \n",
      "\n",
      "user_id              274061  274301  274308  275970  277427  277639  278418  \n",
      "book_title                                                                   \n",
      "1984                    0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
      "1st to Die: A Novel     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
      "2nd Chance              0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
      "4 Blondes               0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
      "A Bend in the Road      0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
      "\n",
      "[5 rows x 810 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The Eight by Katherine Neville',\n",
       " 'A Heartbreaking Work of Staggering Genius by Dave Eggers',\n",
       " 'Bridget Jones: The Edge of Reason by Helen Fielding',\n",
       " \"Drowning Ruth (Oprah's Book Club) by CHRISTINA SCHWARZ\",\n",
       " 'Midwives: A Novel by Chris Bohjalian',\n",
       " 'The Mists of Avalon by MARION ZIMMER BRADLEY',\n",
       " \"The Sweet Potato Queens' Book of Love by JILL CONNER BROWNE\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import pickle\n",
    "from typing import Dict, Text\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Dense, Input\n",
    "\n",
    "# read the csv to memory\n",
    "df = pd.read_csv(\"Preprocessed_data.csv\")\n",
    "df.head()\n",
    "\n",
    "# Drop 'Unnamed' column\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head()\n",
    "\n",
    "# view information about the dataset\n",
    "df.info()\n",
    "df.shape\n",
    "\n",
    "# extracting the required column for the model and web app\n",
    "cleaned_data = df[[\"user_id\", \"book_title\", \"rating\", \"img_l\", \"book_author\"]]\n",
    "\n",
    "# save the new dataset to memory\n",
    "cleaned_data.to_csv(\"filtered_df.csv\", index=False)\n",
    "\n",
    "# Convert the datatypes to TensorFlow datatypes\n",
    "cleaned_data = df[[\"user_id\", \"book_title\", \"rating\", \"book_author\"]].astype({\"user_id\": np.str_, \n",
    "                                                                               \"book_title\": np.str_, \n",
    "                                                                               \"rating\": np.float32, \n",
    "                                                                               \"book_author\": np.str_}\n",
    ")\n",
    "\n",
    "#The tf.data.Dataset API allows for writing descriptive and efficient input pipelines.\n",
    "ratings_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(cleaned_data['user_id'], tf.string),\n",
    "                                                      tf.cast(cleaned_data['book_title'], tf.string),\n",
    "                                                      tf.cast(cleaned_data['rating'], tf.float32),\n",
    "                                                      tf.cast(cleaned_data['book_author'], tf.string)\n",
    "))\n",
    "\n",
    "# assign names to the TensorFlow datatypes\n",
    "ratings = ratings_dataset.map(lambda x0, x1, x2, x3: {\n",
    "    \"user_id\": x0,\n",
    "    \"book_title\": x1,\n",
    "    \"rating\": x2,\n",
    "    \"book_author\": x3\n",
    "})\n",
    "\n",
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)\n",
    "\n",
    "\n",
    "# Building the Model Architechture\n",
    "class RankingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        # Compute embeddings for users.\n",
    "        self.user_embeddings = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # Compute embeddings for books.\n",
    "        self.books_embeddings = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_book_titles, mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_book_titles) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # Computing the predictions.\n",
    "        self.ratings = tf.keras.Sequential([\n",
    "          # Learn multiple dense layers.\n",
    "          tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "          tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "          # Make rating predictions in the final layer.\n",
    "          tf.keras.layers.Dense(1)\n",
    "      ])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        user_id, book_title = inputs\n",
    "\n",
    "        user_embedding = self.user_embeddings(user_id)\n",
    "        book_embedding = self.books_embeddings(book_title)\n",
    "        \n",
    "        return self.ratings(tf.concat([user_embedding, book_embedding], axis=1))\n",
    "    \n",
    "# Reference https://www.tensorflow.org/recommenders/examples/basic_ranking\n",
    "# Reference https://medium.com/@hamza.emra/introduction-to-recommendation-systems-with-tensorflow-recommenders-a116e5e5a940\n",
    "\n",
    "# load the loss function metric computation\n",
    "task = tfrs.tasks.Ranking(\n",
    "  loss = tf.keras.losses.MeanSquaredError(),\n",
    "  metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "\n",
    "# split the dataset for training and testing\n",
    "tf.random.set_seed(1990)\n",
    "shuffled = ratings.shuffle(100_000, seed=1990, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(75_000)\n",
    "test = shuffled.skip(75_000).take(25_000)\n",
    "\n",
    "# get the unique data \n",
    "book_titles = ratings.batch(1_000_000).map(lambda x: x[\"book_title\"])\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_book_titles = np.unique(np.concatenate(list(book_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "# save the unique data to memory\n",
    "with open(\"unique_book_titles.pkl\", \"wb\") as f:\n",
    "    pickle.dump(unique_book_titles, f)\n",
    "    \n",
    "with open(\"unique_user_ids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(unique_user_ids, f)\n",
    "    \n",
    "\n",
    "# using TensorFlow libraries to build model\n",
    "class BookModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ranking_model: tf.keras.Model = RankingModel()\n",
    "        self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "    def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        return self.ranking_model(\n",
    "        (features[\"user_id\"], features[\"book_title\"]))\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        labels = features.pop(\"rating\")\n",
    "    \n",
    "        rating_predictions = self(features)\n",
    "\n",
    "        # The task computes the loss and the metrics.\n",
    "        return self.task(labels=labels, predictions=rating_predictions)\n",
    "    \n",
    "# training and compilation\n",
    "model = BookModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "# Shuffle the training data and create batches for efficient training\n",
    "train_data = train.shuffle(len(train)).batch(256).cache().take(100_000)\n",
    "\n",
    "# Create batches for the test data and cache them for faster access\n",
    "test_data = test.batch(256).cache()\n",
    "\n",
    "# Train the model using the prepared training data and evaluate on the test data\n",
    "model.fit(train_data, epochs=25, validation_data=test_data)\n",
    "\n",
    "# fitting the model\n",
    "model.evaluate(test_data, return_dict=True)\n",
    "\n",
    "# model testing\n",
    "test_ratings = {}\n",
    "for book_title in unique_book_titles[:15]:\n",
    "      test_ratings[book_title.decode(\"utf-8\")] = model({\n",
    "      \"user_id\": np.array([\"15\"]),\n",
    "      \"book_title\": np.array([book_title])\n",
    "  })\n",
    "\n",
    "# print the predicted ratings\n",
    "for title, score in sorted(test_ratings.items(), key=lambda x: x[1], reverse=True):\n",
    "  print(f\"{title}: {score}\")\n",
    "\n",
    "\n",
    "# using cosine similarity\n",
    "filtered_data = pd.read_csv('filtered_df.csv')\n",
    "df = filtered_data.copy()\n",
    "\n",
    "# Step 1: Identify users with more than 200 ratings\n",
    "x = df.groupby('user_id').count()['rating'] > 200\n",
    "similar_users = x[x].index\n",
    "\n",
    "# Step 2: Filter ratings data to include only ratings from similar users\n",
    "filtered_rating = df[df['user_id'].isin(similar_users)]\n",
    "\n",
    "# Step 3: Identify books with 50 or more ratings\n",
    "y = filtered_rating.groupby('book_title').count()['rating'] >= 50\n",
    "famous_books = y[y].index\n",
    "\n",
    "# Step 4: Filter ratings data to include only ratings for famous books\n",
    "final_ratings = filtered_rating[filtered_rating['book_title'].isin(famous_books)]\n",
    "\n",
    "pt = final_ratings.pivot_table(index='book_title', columns='user_id', values='rating')\n",
    "\n",
    "pt.fillna(0,inplace=True)\n",
    "print(pt.head())\n",
    "\n",
    "# Calculate similarity scores using cosine similarity\n",
    "similarity_scores = cosine_similarity(pt)\n",
    "\n",
    "def recommend(book_title, pt, similarity_scores, df):\n",
    "    \n",
    "    # Find index of the input book\n",
    "    index = np.where(pt.index == book_title)[0][0]\n",
    "\n",
    "    # Sort similar items by similarity score and select top recommendations\n",
    "    similar_items = sorted(\n",
    "        ((i, score) for i, score in enumerate(similarity_scores[index])),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[1:8]  # Only considering the top 7 similar items\n",
    "\n",
    "    # Initialize a list to store recommended books\n",
    "    recommended_books = []\n",
    "\n",
    "    # Loop through the similar items and gather book information for recommendations\n",
    "    for i, _ in similar_items:\n",
    "        # Filter the DataFrame to get information about the recommended book\n",
    "        temp_df = df[df['book_title'] == pt.index[i]]\n",
    "        book_info = temp_df.drop_duplicates('book_title')[['book_title', \"book_author\"]].values[0]\n",
    "        recommended_books.append(f\"{book_info[0]} by {book_info[1]}\")\n",
    "\n",
    "    # Return the list of recommended books\n",
    "    return recommended_books\n",
    "\n",
    "# input the book to recommend\n",
    "recommend(\"Year of Wonders\", pt, similarity_scores, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ed1b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as ranking_layer_call_fn, ranking_layer_call_and_return_conditional_losses, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "tf.saved_model.save(model, \"export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1f94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
