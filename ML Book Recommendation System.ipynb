{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817dcc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1031175 entries, 0 to 1031174\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count    Dtype  \n",
      "---  ------               --------------    -----  \n",
      " 0   user_id              1031175 non-null  int64  \n",
      " 1   location             1031175 non-null  object \n",
      " 2   age                  1031175 non-null  float64\n",
      " 3   isbn                 1031175 non-null  object \n",
      " 4   rating               1031175 non-null  int64  \n",
      " 5   book_title           1031175 non-null  object \n",
      " 6   book_author          1031175 non-null  object \n",
      " 7   year_of_publication  1031175 non-null  float64\n",
      " 8   publisher            1031175 non-null  object \n",
      " 9   img_s                1031175 non-null  object \n",
      " 10  img_m                1031175 non-null  object \n",
      " 11  img_l                1031175 non-null  object \n",
      " 12  Summary              1031175 non-null  object \n",
      " 13  Language             1031175 non-null  object \n",
      " 14  Category             1031175 non-null  object \n",
      " 15  city                 1017072 non-null  object \n",
      " 16  state                1008377 non-null  object \n",
      " 17  country              995801 non-null   object \n",
      "dtypes: float64(2), int64(2), object(14)\n",
      "memory usage: 141.6+ MB\n",
      "{'book_author': b'Mark P. O. Morford',\n",
      " 'book_title': b'Classical Mythology',\n",
      " 'rating': 0.0,\n",
      " 'user_id': b'2'}\n",
      "Epoch 1/25\n",
      "293/293 [==============================] - 6s 12ms/step - root_mean_squared_error: 3.8150 - loss: 14.5497 - regularization_loss: 0.0000e+00 - total_loss: 14.5497 - val_root_mean_squared_error: 3.6963 - val_loss: 13.5840 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13.5840\n",
      "Epoch 2/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 3.3769 - loss: 11.4025 - regularization_loss: 0.0000e+00 - total_loss: 11.4025 - val_root_mean_squared_error: 3.8793 - val_loss: 15.3227 - val_regularization_loss: 0.0000e+00 - val_total_loss: 15.3227\n",
      "Epoch 3/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 2.9745 - loss: 8.8457 - regularization_loss: 0.0000e+00 - total_loss: 8.8457 - val_root_mean_squared_error: 3.9029 - val_loss: 14.8454 - val_regularization_loss: 0.0000e+00 - val_total_loss: 14.8454\n",
      "Epoch 4/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 2.6321 - loss: 6.9270 - regularization_loss: 0.0000e+00 - total_loss: 6.9270 - val_root_mean_squared_error: 4.0332 - val_loss: 15.7213 - val_regularization_loss: 0.0000e+00 - val_total_loss: 15.7213\n",
      "Epoch 5/25\n",
      "293/293 [==============================] - 2s 5ms/step - root_mean_squared_error: 2.3966 - loss: 5.7433 - regularization_loss: 0.0000e+00 - total_loss: 5.7433 - val_root_mean_squared_error: 4.1225 - val_loss: 16.0509 - val_regularization_loss: 0.0000e+00 - val_total_loss: 16.0509\n",
      "Epoch 6/25\n",
      "293/293 [==============================] - 2s 5ms/step - root_mean_squared_error: 2.2626 - loss: 5.1199 - regularization_loss: 0.0000e+00 - total_loss: 5.1199 - val_root_mean_squared_error: 4.1622 - val_loss: 16.1522 - val_regularization_loss: 0.0000e+00 - val_total_loss: 16.1522\n",
      "Epoch 7/25\n",
      "293/293 [==============================] - 2s 5ms/step - root_mean_squared_error: 2.1710 - loss: 4.7145 - regularization_loss: 0.0000e+00 - total_loss: 4.7145 - val_root_mean_squared_error: 4.1844 - val_loss: 15.9840 - val_regularization_loss: 0.0000e+00 - val_total_loss: 15.9840\n",
      "Epoch 8/25\n",
      "293/293 [==============================] - 2s 5ms/step - root_mean_squared_error: 2.0920 - loss: 4.3780 - regularization_loss: 0.0000e+00 - total_loss: 4.3780 - val_root_mean_squared_error: 4.3312 - val_loss: 16.8964 - val_regularization_loss: 0.0000e+00 - val_total_loss: 16.8964\n",
      "Epoch 9/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 2.0326 - loss: 4.1323 - regularization_loss: 0.0000e+00 - total_loss: 4.1323 - val_root_mean_squared_error: 4.3512 - val_loss: 17.5345 - val_regularization_loss: 0.0000e+00 - val_total_loss: 17.5345\n",
      "Epoch 10/25\n",
      "293/293 [==============================] - 2s 5ms/step - root_mean_squared_error: 1.9698 - loss: 3.8810 - regularization_loss: 0.0000e+00 - total_loss: 3.8810 - val_root_mean_squared_error: 4.4131 - val_loss: 18.0901 - val_regularization_loss: 0.0000e+00 - val_total_loss: 18.0901\n",
      "Epoch 11/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.9243 - loss: 3.7026 - regularization_loss: 0.0000e+00 - total_loss: 3.7026 - val_root_mean_squared_error: 4.4628 - val_loss: 18.9372 - val_regularization_loss: 0.0000e+00 - val_total_loss: 18.9372\n",
      "Epoch 12/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.8859 - loss: 3.5562 - regularization_loss: 0.0000e+00 - total_loss: 3.5562 - val_root_mean_squared_error: 4.5376 - val_loss: 19.5579 - val_regularization_loss: 0.0000e+00 - val_total_loss: 19.5579\n",
      "Epoch 13/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.8486 - loss: 3.4172 - regularization_loss: 0.0000e+00 - total_loss: 3.4172 - val_root_mean_squared_error: 4.5625 - val_loss: 19.9670 - val_regularization_loss: 0.0000e+00 - val_total_loss: 19.9670\n",
      "Epoch 14/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.7979 - loss: 3.2327 - regularization_loss: 0.0000e+00 - total_loss: 3.2327 - val_root_mean_squared_error: 4.6407 - val_loss: 20.7482 - val_regularization_loss: 0.0000e+00 - val_total_loss: 20.7482\n",
      "Epoch 15/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.7600 - loss: 3.0967 - regularization_loss: 0.0000e+00 - total_loss: 3.0967 - val_root_mean_squared_error: 4.6276 - val_loss: 20.8029 - val_regularization_loss: 0.0000e+00 - val_total_loss: 20.8029\n",
      "Epoch 16/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.7336 - loss: 3.0048 - regularization_loss: 0.0000e+00 - total_loss: 3.0048 - val_root_mean_squared_error: 4.6653 - val_loss: 21.0165 - val_regularization_loss: 0.0000e+00 - val_total_loss: 21.0165\n",
      "Epoch 17/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.7013 - loss: 2.8937 - regularization_loss: 0.0000e+00 - total_loss: 2.8937 - val_root_mean_squared_error: 4.6479 - val_loss: 20.8929 - val_regularization_loss: 0.0000e+00 - val_total_loss: 20.8929\n",
      "Epoch 18/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.6710 - loss: 2.7917 - regularization_loss: 0.0000e+00 - total_loss: 2.7917 - val_root_mean_squared_error: 4.6749 - val_loss: 21.7624 - val_regularization_loss: 0.0000e+00 - val_total_loss: 21.7624\n",
      "Epoch 19/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.6236 - loss: 2.6355 - regularization_loss: 0.0000e+00 - total_loss: 2.6355 - val_root_mean_squared_error: 4.6998 - val_loss: 21.5497 - val_regularization_loss: 0.0000e+00 - val_total_loss: 21.5497\n",
      "Epoch 20/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.5849 - loss: 2.5118 - regularization_loss: 0.0000e+00 - total_loss: 2.5118 - val_root_mean_squared_error: 4.7258 - val_loss: 22.1058 - val_regularization_loss: 0.0000e+00 - val_total_loss: 22.1058\n",
      "Epoch 21/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.5588 - loss: 2.4298 - regularization_loss: 0.0000e+00 - total_loss: 2.4298 - val_root_mean_squared_error: 4.7236 - val_loss: 21.8120 - val_regularization_loss: 0.0000e+00 - val_total_loss: 21.8120\n",
      "Epoch 22/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.5266 - loss: 2.3307 - regularization_loss: 0.0000e+00 - total_loss: 2.3307 - val_root_mean_squared_error: 4.7258 - val_loss: 22.1493 - val_regularization_loss: 0.0000e+00 - val_total_loss: 22.1493\n",
      "Epoch 23/25\n",
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.4820 - loss: 2.1963 - regularization_loss: 0.0000e+00 - total_loss: 2.1963 - val_root_mean_squared_error: 4.7883 - val_loss: 21.9461 - val_regularization_loss: 0.0000e+00 - val_total_loss: 21.9461\n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/293 [==============================] - 1s 5ms/step - root_mean_squared_error: 1.4453 - loss: 2.0896 - regularization_loss: 0.0000e+00 - total_loss: 2.0896 - val_root_mean_squared_error: 4.7466 - val_loss: 22.6622 - val_regularization_loss: 0.0000e+00 - val_total_loss: 22.6622\n",
      "Epoch 25/25\n",
      "293/293 [==============================] - 2s 5ms/step - root_mean_squared_error: 1.4148 - loss: 2.0019 - regularization_loss: 0.0000e+00 - total_loss: 2.0019 - val_root_mean_squared_error: 4.8303 - val_loss: 22.8045 - val_regularization_loss: 0.0000e+00 - val_total_loss: 22.8045\n",
      "98/98 [==============================] - 0s 3ms/step - root_mean_squared_error: 4.8303 - loss: 23.3249 - regularization_loss: 0.0000e+00 - total_loss: 23.3249\n",
      " God's Little Promise Book: [[6.4315186]]\n",
      " Flight of Fancy: American Heiresses (Zebra Ballad Romance): [[5.7917256]]\n",
      " Final Fantasy Anthology: Official Strategy Guide (Brady Games): [[5.790288]]\n",
      " Always Have Popsicles: [[5.636463]]\n",
      " Deceived: [[5.354292]]\n",
      " Ask Lily (Young Women of Faith: Lily Series, Book 5): [[5.1199327]]\n",
      " Dark Justice: [[5.092329]]\n",
      " Earth Prayers From around the World: 365 Prayers, Poems, and Invocations for Honoring the Earth: [[4.9684796]]\n",
      " Beyond IBM: Leadership Marketing and Finance for the 1990s: [[4.9162335]]\n",
      " Clifford Visita El Hospital (Clifford El Gran Perro Colorado): [[4.893115]]\n",
      " A Light in the Storm: The Civil War Diary of Amelia Martin, Fenwick Island, Delaware, 1861 (Dear America): [[4.799531]]\n",
      " Good Wives: Image and Reality in the Lives of Women in Northern New England, 1650-1750: [[4.6948276]]\n",
      " Apple Magic (The Collector's series): [[4.606384]]\n",
      " Garfield Bigger and Better (Garfield (Numbered Paperback)): [[4.5924864]]\n",
      " Goosebumps Monster Edition 1: Welcome to Dead House, Stay Out of the Basement, and Say Cheese and Die!: [[4.2163286]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as ranking_10_layer_call_fn, ranking_10_layer_call_and_return_conditional_losses, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id              254     2276    2766    2977    3363    4017    4385    \\\n",
      "book_title                                                                    \n",
      "1984                    9.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1st to Die: A Novel     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2nd Chance              0.0    10.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4 Blondes               0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "A Bend in the Road      0.0     0.0     7.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "user_id              6251    6323    6543    ...  271705  273979  274004  \\\n",
      "book_title                                   ...                           \n",
      "1984                    0.0     0.0     0.0  ...    10.0     0.0     0.0   \n",
      "1st to Die: A Novel     0.0     0.0     9.0  ...     0.0     0.0     0.0   \n",
      "2nd Chance              0.0     0.0     0.0  ...     0.0     0.0     0.0   \n",
      "4 Blondes               0.0     0.0     0.0  ...     0.0     0.0     0.0   \n",
      "A Bend in the Road      0.0     0.0     0.0  ...     0.0     0.0     0.0   \n",
      "\n",
      "user_id              274061  274301  274308  275970  277427  277639  278418  \n",
      "book_title                                                                   \n",
      "1984                    0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
      "1st to Die: A Novel     0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
      "2nd Chance              0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
      "4 Blondes               0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
      "A Bend in the Road      0.0     0.0     0.0     0.0     0.0     0.0     0.0  \n",
      "\n",
      "[5 rows x 810 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The Eight by 2276',\n",
       " 'A Heartbreaking Work of Staggering Genius by 1161',\n",
       " 'Bridget Jones: The Edge of Reason by 254',\n",
       " \"Drowning Ruth (Oprah's Book Club) by 5737\",\n",
       " 'Midwives: A Novel by 2313',\n",
       " 'The Mists of Avalon by 254',\n",
       " \"The Sweet Potato Queens' Book of Love by 5737\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import pickle\n",
    "from typing import Dict, Text\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Dense, Input\n",
    "\n",
    "# read the csv to memory\n",
    "df = pd.read_csv(\"Preprocessed_data.csv\")\n",
    "df.head()\n",
    "\n",
    "# Drop 'Unnamed' column\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head()\n",
    "\n",
    "# view information about the dataset\n",
    "df.info()\n",
    "df.shape\n",
    "\n",
    "# extracting the required column for the model and web app\n",
    "cleaned_data = df[[\"user_id\", \"book_title\", \"rating\", \"img_l\", \"book_author\"]]\n",
    "\n",
    "# save the new dataset to memory\n",
    "cleaned_data.to_csv(\"filtered_df.csv\", index=False)\n",
    "\n",
    "# Convert the datatypes to TensorFlow datatypes\n",
    "cleaned_data = df[[\"user_id\", \"book_title\", \"rating\", \"book_author\"]].astype({\"user_id\": np.str_, \n",
    "                                                                               \"book_title\": np.str_, \n",
    "                                                                               \"rating\": np.float32, \n",
    "                                                                               \"book_author\": np.str_}\n",
    ")\n",
    "\n",
    "#The tf.data.Dataset API allows for writing descriptive and efficient input pipelines.\n",
    "ratings_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(cleaned_data['user_id'], tf.string),\n",
    "                                                      tf.cast(cleaned_data['book_title'], tf.string),\n",
    "                                                      tf.cast(cleaned_data['rating'], tf.float32),\n",
    "                                                      tf.cast(cleaned_data['book_author'], tf.string)\n",
    "))\n",
    "\n",
    "# assign names to the TensorFlow datatypes\n",
    "ratings = ratings_dataset.map(lambda x0, x1, x2, x3: {\n",
    "    \"user_id\": x0,\n",
    "    \"book_title\": x1,\n",
    "    \"rating\": x2,\n",
    "    \"book_author\": x3\n",
    "})\n",
    "\n",
    "for x in ratings.take(1).as_numpy_iterator():\n",
    "  pprint.pprint(x)\n",
    "\n",
    "# split the dataset for training and testing\n",
    "tf.random.set_seed(1990)\n",
    "shuffled = ratings.shuffle(100_000, seed=1990, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(75_000)\n",
    "test = shuffled.skip(75_000).take(25_000)\n",
    "\n",
    "# get the unique data \n",
    "book_titles = ratings.batch(1_000_000).map(lambda x: x[\"book_title\"])\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_book_titles = np.unique(np.concatenate(list(book_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "\n",
    "# save the unique data to memory\n",
    "with open(\"unique_book_titles.pkl\", \"wb\") as f:\n",
    "    pickle.dump(unique_book_titles, f)\n",
    "    \n",
    "with open(\"unique_user_ids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(unique_user_ids, f)\n",
    "    \n",
    "\n",
    "# Building the Model Architechture\n",
    "class RankingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        # Compute embeddings for users.\n",
    "        self.user_embeddings = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # Compute embeddings for books.\n",
    "        self.books_embeddings = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_book_titles, mask_token=None),\n",
    "          tf.keras.layers.Embedding(len(unique_book_titles) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # Compute predictions.\n",
    "        self.ratings = tf.keras.Sequential([\n",
    "          # Learn multiple dense layers.\n",
    "          tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "          tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "          # Make rating predictions in the final layer.\n",
    "          tf.keras.layers.Dense(1)\n",
    "      ])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        user_id, book_title = inputs\n",
    "\n",
    "        user_embedding = self.user_embeddings(user_id)\n",
    "        book_embedding = self.books_embeddings(book_title)\n",
    "        \n",
    "        return self.ratings(tf.concat([user_embedding, book_embedding], axis=1))\n",
    "    \n",
    "# Reference https://www.tensorflow.org/recommenders/examples/basic_ranking\n",
    "# Reference https://medium.com/@hamza.emra/introduction-to-recommendation-systems-with-tensorflow-recommenders-a116e5e5a940\n",
    "\n",
    "# load the loss function metric computation\n",
    "task = tfrs.tasks.Ranking(\n",
    "  loss = tf.keras.losses.MeanSquaredError(),\n",
    "  metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "\n",
    "# using TensorFlow libraries to build model\n",
    "class BookModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ranking_model: tf.keras.Model = RankingModel()\n",
    "        self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "    def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        return self.ranking_model(\n",
    "        (features[\"user_id\"], features[\"book_title\"]))\n",
    "\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        labels = features.pop(\"rating\")\n",
    "    \n",
    "        rating_predictions = self(features)\n",
    "\n",
    "        # The task computes the loss and the metrics.\n",
    "        return self.task(labels=labels, predictions=rating_predictions)\n",
    "    \n",
    "# training and compilation\n",
    "model = BookModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "train_data = train.shuffle(len(train)).batch(256).cache().take(100_000)\n",
    "test_data = test.batch(256).cache()\n",
    "\n",
    "model.fit(train_data, epochs=25, validation_data=test_data)\n",
    "\n",
    "# fitting the model\n",
    "model.evaluate(test_data, return_dict=True)\n",
    "\n",
    "# model testing\n",
    "test_ratings = {}\n",
    "for book_title in unique_book_titles[:15]:\n",
    "      test_ratings[book_title.decode(\"utf-8\")] = model({\n",
    "      \"user_id\": np.array([\"15\"]),\n",
    "      \"book_title\": np.array([book_title])\n",
    "  })\n",
    "\n",
    "for title, score in sorted(test_ratings.items(), key=lambda x: x[1], reverse=True):\n",
    "  print(f\"{title}: {score}\")\n",
    "\n",
    "# save the model\n",
    "tf.saved_model.save(model, \"export\")\n",
    "\n",
    "#loading the model to confirm functionality\n",
    "loaded = tf.saved_model.load(\"export\")\n",
    "\n",
    "loaded({\"user_id\": np.array([\"15\"]), \n",
    "        \"book_title\":np.array([\"Dark Justice\"]), \n",
    "        \"book_author\":np.array([\"Richard Bruce Wright\"])\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# using cosine similarity\n",
    "#filtered_data = pd.read_csv('filtered_df.csv')\n",
    "df = filtered_data.copy()\n",
    "\n",
    "# Step 1: Identify users with more than 200 ratings\n",
    "x = df.groupby('user_id').count()['rating'] > 200\n",
    "similar_users = x[x].index\n",
    "\n",
    "# Step 2: Filter ratings data to include only ratings from similar users\n",
    "filtered_rating = df[df['user_id'].isin(similar_users)]\n",
    "\n",
    "# Step 3: Identify books with 50 or more ratings\n",
    "y = filtered_rating.groupby('book_title').count()['rating'] >= 50\n",
    "famous_books = y[y].index\n",
    "\n",
    "# Step 4: Filter ratings data to include only ratings for famous books\n",
    "final_ratings = filtered_rating[filtered_rating['book_title'].isin(famous_books)]\n",
    "\n",
    "pt = final_ratings.pivot_table(index='book_title', columns='user_id', values='rating')\n",
    "\n",
    "pt.fillna(0,inplace=True)\n",
    "print(pt.head())\n",
    "\n",
    "# Calculate similarity scores using cosine similarity\n",
    "similarity_scores = cosine_similarity(pt)\n",
    "\n",
    "def recommend(book_title, pt, similarity_scores, df):\n",
    "    \n",
    "    # Find index of the input book\n",
    "    index = np.where(pt.index == book_title)[0][0]\n",
    "\n",
    "    # Sort similar items by similarity score and select top recommendations\n",
    "    similar_items = sorted(\n",
    "        ((i, score) for i, score in enumerate(similarity_scores[index])),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[1:8]  # Only considering the top 7 similar items\n",
    "\n",
    "    # Initialize a list to store recommended books\n",
    "    recommended_books = []\n",
    "\n",
    "    # Loop through the similar items and gather book information for recommendations\n",
    "    for i, _ in similar_items:\n",
    "        # Filter the DataFrame to get information about the recommended book\n",
    "        temp_df = df[df['book_title'] == pt.index[i]]\n",
    "        book_info = temp_df.drop_duplicates('book_title')[['book_title', \"user_id\"]].values[0]\n",
    "        recommended_books.append(f\"{book_info[0]} by {book_info[1]}\")\n",
    "\n",
    "    # Return the list of recommended books\n",
    "    return recommended_books\n",
    "\n",
    "recommend(\"Year of Wonders\", pt, similarity_scores, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd0693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
